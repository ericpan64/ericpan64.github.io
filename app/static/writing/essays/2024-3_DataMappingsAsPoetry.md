# Data Mapping is Poetry (not Plumbing)

Eric S Pan

[CS343D Essay](https://cs343d.github.io/) (Win 24’) [[^0]](#footnotes)

At my last full-time role, I worked as a Senior Software Engineer on the data interoperability team, and our core work involved writing the transformations from our custom data model into a standards-compliant API. A teammate light-heartingly likened the role of the interoperability team to data plumbing. While parts of the analogy felt right, something about it bothered me in ways that weren’t obvious. In this essay, I reflect more deeply and outline: 1) the core intuition on why data mappings are viewed more like plumbing, 2) how reasonable and subtle viewpoints promote this, and 3) the promising implications of viewing data mappings as poetry which can align all parties and bring more elegance to the world.

## I. Opinion: Data Engineering (and Science) is about mapping semantics

Data Engineering (and Data Science) is a relatively new and ambiguous field. Through years of career exploration and reading job descriptions, I’ve noticed that the descriptions of “Data Engineer” and “Data Scientist” fluctuate with ambiguous boundaries. Once I was hired as a “Data Informatics Engineer” and subsequently spent most of my day begrudgingly configuring [AWS EC2](https://aws.amazon.com/pm/ec2/) instances and [Apache NiFi](https://nifi.apache.org/) GUIs. It seems like there’s a clear desire for, though an inconsistent understanding of, what these roles entail.

The focus of this essay will be on the “Data Engineer”, though let’s start with “Data Scientist” which is the “hotter” term [[^1]](#footnotes). In undergrad, I was fascinated by the beauty behind abstract mathematical thinking, though I cared mostly about understanding how to apply it in industry and research – I studied Bioengineering and cared most about the semantics of human health. It was \~2016, and “Data Scientist” was a new and popular role that seemed to bridge semantic knowledge with industry applications through math and statistics. A description that I understood was that a Data Scientist focuses on “making sense of data” whether that be through statistical analysis, dashboards, experiments, and practical root-cause analysis. After some failed interviews and a short internship at a Series D startup, I concluded that no one had one clear answer, though there were some clear heuristics around understanding data.

What I ended-up getting into, and the focus of this essay, is Data Engineering which I’d consider the unofficial “blue-collar” variant of Data Science. While a Data Scientist might focus on semantic understanding, the Data Engineer focuses more on the nitty-gritty of organizing data: converting between data types, conforming to standards, running ETLs – ultimately making sure things end up in the right format at the right time for other engineers and scientists to build on. Informally, Data Engineers hold roles closer to technical consulting as the dedicated Data Engineer must leverage no- or low-code tools to wrangle data, and often building your own software tool is uncommon since then the title would be “Software Engineer”. Interestingly, many Data Scientists are forced to spend significant parts of their day-to-day on data engineering. Usually then, the Data Scientist reasonably learns just enough to get their job done, often informally and begrudgingly since most statisticians and domain experts care more about semantics and less about code (just as most Software Engineers care less about statistics). Many terms are thrown around, though here we’ll mainly use “data mapping”.

Here I’d like to present a possibly novel idea: data mapping is really about *mapping semantics*, more akin to mapping abstract spaces in math, and actualized through practical media like a domain-specific language. Data is simply how those semantics are captured, and afterwards the process of wrangling and mapping is what provides the opportunity to inject semantic knowledge from the physical to digital worlds [[^2]](#footnotes). However, since data engineering is often approached with reluctance due to misaligned skillsets and ambiguous scoping, it’s unappealing to the scientific mind yet essential to actualize these ideas. I believe there’s a way to bridge this gap by approaching the discipline with empathy, collaboration, and elegance.

## II. Motivation: Most of “Data-Driven” Applications is “Data Wrangling”

One of the pioneers of AI, Andrew Ng, has noted the importance of [data engineering for data-centric AI](https://mitsloan.mit.edu/ideas-made-to-matter/why-its-time-data-centric-artificial-intelligence) or what I'll informally call "data wrangling". Unless you are designing new algorithms, the application of ML and AI is effectively treated as a “black box” – effectively an input-output machine. To have consistent and reliable results, there’s a strong need for evaluation frameworks, especially as we move to an LLM-frenzied world as evidenced by [developer talks by OpenAI](https://youtu.be/XGJNo8TpuVA?si=3suROaaTeITT1BkI) and research groups that I’ve gotten to participate in at Stanford ([Shahlab](https://shahlab.stanford.edu/), [HealthRex](https://www.healthrexlab.com/)). To broadly believe in the future promise of ML and AI means to believe in the advances that will come from aggregating, organizing, and “wrangling” data by whatever means necessary.

“Data wrangling” is exactly what it sounds like: not fun\! Nevertheless, it’s an essential part of actually deploying and making ML and AI usable. Additionally, if these models are essentially “black boxes” and left to wrangle on their own accords, how else can we achieve observability other than through how we wrangle the training data and interpret the outputs? For most, this is where the conversation ends. Yet this begs the further question: what are the semantics of how the training data gets wrangled, and what semantic assumptions went into its creation? 

Similar to the argument that we’re close to [no more “free” performance from Moore’s Law](https://www.science.org/doi/10.1126/science.aam9744), we may reach a similar state where the gains from “big data” becomes less important than *meaningful, semantic data*: an idea behind the [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web) and related research on ontologies and metadata [[^3]](#footnotes). If data is the new oil, then semantic data might be *gold*. These efforts are noble though difficult to motivate since, unlike gold, the solution lacks clear, immediate return. In practice, actualization of these semantics is with data mappings: much of which is done begrudgingly and in isolation in-part due to the complexity of collaboration.

## III. The reluctant Data Engineer: A lonely and obfuscated journey

The goal of data mapping is to prepare data for a separate semantic problem. This implies two subproblems: 1\) writing data mappings is *not* most peoples’ first choice (e.g. a begrudging Data Scientist), and 2\) someone with some semantic understanding needs to work on it. The task itself also requires skills in solving two additional subproblems: the *syntactic* (i.e. is the format correct) and *systematic* (i.e. did things run smoothly) problems, thus leaving a very small pool of people with all the prerequisites. Reasonably so, interdisciplinary teams form and coordinate to work on data mappings and ultimately the semantic problem at hand.

Let’s define a common variation of this problem: converting a dict-like structure (the “source”) into another (the “result”). Here, a dict-like structure is a key-value store object, and let’s assume we need to write some code to “wrangle” this object. The details vary based on the programming language, though it’s a near-universal abstraction and commonly occurs across codebases. Most basically we can rearrange elements from the source, and there’s also a clear opportunity to add, modify, and combine multiple sources to achieve the desired result.  Even for the beginner programmer, the general idea isn’t too complicated, and often the code only needs to work for a single, specific case.

The simplicity of this problem provides a subtle and intrusive opportunity: write quick and verbose code (reasonably so) that does simple things but possibly overwhelms a non-programmer based on sheer volume of text (hopefully unintentionally). This is a clear downside when working alone or teaching, though when working in interdisciplinary teams, this obfuscation might provide defensibility against less technical team members who understand the semantics and defer on syntactic and systematic domains. Without the ability to reason with the code or data, less technical team members can only assume, check sources and results, and hope for the best since the system is effectively a “black box”. When results seem off, one approach is confrontation of the Data Engineer, though in the face of verbose code and unclear outcomes, this often isn’t a battle worth starting unprompted. It’s also reasonable for the often reluctant Data Engineer to be defensive over existing code without clear and careful feedback — many would prefer to be doing something else — and this is especially tricky when wrangling with custom and often temporary abstractions meant to be understandably quick and one-off. This dynamic and the context of being forced into data wrangling can make an already difficult problem of communicating across domains even more challenging. 

## IV. Interoperability: A machine’s need for connection and community

My career (to this point) has been focused on healthcare data interoperability. Interoperability focuses on getting systems to communicate, or “talk”, with one another effectively. This is a common problem in healthcare since there are many more distinct entities involved in a system. Traditionally, a few software vendors aggregate most of the data (e.g. electronic health records) and reasonably aren’t incentivized to prioritize sharing over product features, thus leading to repeat tasks and an inability to meaningfully build efficient and collaborative systems. In the US, the government continues to develop and enforce [legislation to improve interoperability](https://www.healthit.gov/topic/oncs-cures-act-final-rule). 

Many are very excited about achieving interoperability, however many don’t care about the details and would prefer to focus on their own semantic domains (sound familiar?). The unit of interoperability, I’d argue, is indeed data mappings – bridging captured data and semantics. Everyone wants it done, and most aren’t particularly excited to actually do it. As an analogy, it would be efficient if all people spoke English, though it’s quite a feat to suggest someone else learn English from scratch, and abandoning other languages also loses important cultural expressions and nuances. Interoperability, like language, signals a greater need for community and collaboration. In ways, machines draw similarities to the people interacting with them.

## V. Data Standards: The “little languages” of semantics

Standardization is clearly needed to achieve any level of interoperability. This has allowed for marvelous systems and mind-boggling developments in other computing domains like networking (e.g. [TCP/IP](https://en.wikipedia.org/wiki/Internet_protocol_suite)) and cryptography (e.g. [AES](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)) among many other examples. To achieve this, many successful computing standards have crafted deterministic scenarios to provide consistency. However, many semantic topics are difficult to translate deterministically, thus how might we balance consistency with expressiveness?

The equivalent in the world of data mappings is data standards – technical and semantic “[little languages](https://dl.acm.org/doi/10.1145/6424.315691)” that standardize where data should go and what it should mean. Different standards serve different use-cases: for example in healthcare, [HL7 FHIR](https://hl7.org/fhir/) focuses on atomic healthcare interactions, while standards like [OHDSI OMOP](https://ohdsi.github.io/CommonDataModel/index.html) focus on population-level insights. These standards evolve over time as ideas evolve and learnings are applied to better shape the data models to match the semantics that practitioners want to reason about. Like all standards, they are reactive and evolve to best fit the given times, and thus data standards require feedback from semantic experts leading to large interdisciplinary teams and meetings. Many noble teams have built incredible infrastructure for us to continue building on, and it’s easy to say this kind of difficult work may go unnoticed or underappreciated as systems run “automagically”.

## VI. Data Mappings as Plumbing, or something more?

Data engineering has traditionally felt more “blue collar” since people treat it as the prerequisite to more desirable (i.e. “white collar”) semantic work. If the goal is to reasonably abstract-away the responsibility of syntactic and systematic mapping, we can treat it as a “black box” – similar to how it’s easy to forget about the complex systems of sewage and sanitation that keep society, well, healthy and sane. Unpleasant chaos may ensue if pipes malfunction, just as unpleasant exceptions and results may come from poor data engineering and mapping.

Besides being digital instead of physical, why might data mappings differ from plumbing? For the same reason: data mappings are meant to transport digital *gold*, whereas plumbing is meant to transport physical waste. For data mappings, the unit of transfer is peoples’ *ideas* and semantics they often care deeply about. This also raises a valid concern as future systems rely heavily on the “black boxes” of ML and AI: if we feed garbage-in, we’ll get garbage out (or worse: first gold-to-garbage, then garbage-in garbage-out). Yet we definitely shouldn’t shame nor blame the Data Engineer that is trying their best – it’s hard enough to collaborate, if not also justify caring in the first place. There are clearly many opportunities for better, more collaborative, and more productive approaches.

## VII. A Path Forward: Composability and Usability through Functional and Readable Code

Data mapping code is often difficult to reuse, even when converting from similar data standards that have wrangled a lot of the semantic and programmatic complexity. There are often subtle assumptions made that are difficult to communicate across semantic and programmatic domains. Additionally, since most mapping code is treated as an afterthought as people justifiably get excited about semantic analysis, it’s difficult to justify both attention and proper care and maintainability for a select few and limited use-cases. One solution seems compelling: can we make it easier to reuse data mappings and collaboratively contribute to them as well? 

There are two key properties that are difficult, but possible, to design for: *composability* and *usability*. *Composability* represents the ability to reuse and build on existing abstractions – something that can be implemented with [functional programming principles](https://en.wikipedia.org/wiki/Function_composition_(computer_science)). If there was a way to openly anchor and record past semantic discussions within data mappings, it would also be easier to promote openness and collaboration so we can lighten the burden for the next data wrangler. *Usability* represents the experience of the person interpreting and/or writing the mapping itself. Given that a lot of these people want to focus on semantics, code that is readable and matches their semantic mental model is an appealing approach for a better user experience. In truth, both (and more) are needed for a compelling new solution which is hard. 

[React](https://react.dev/) pioneered a lot of similar concepts to above: developer usability can be improved with functional components, locality of semantics, and ambitious engineers backed by an open community. Interestingly, the idea of JSX (which allows for composability) was [generally rejected at the beginning](https://youtu.be/8pDqJVdNa44?si=3sW71zjMvfv7F98i&t=2207). Even SQL was controversial for its time, though [its origin story](https://dl.acm.org/doi/abs/10.1145/362384.362685) operated behind a key principle: by making systems more *usable*, we can establish *shared consistency* and thus *collaborate effectively* at scale (while saving time and money for everyone). SQL is the [current dominant standard for querying databases](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-databases), and React is the [current dominant library for building frontend applications](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-web-frameworks-and-technologies). To my knowledge, there still isn’t a dominant library for data mapping – everyone just does their own thing, possibly begrudgingly.

## VIII. Data Mapping as Poetry (not Plumbing)

Not many people care about beautiful plumbing, though we can definitely appreciate the hard work that goes into it (whenever we think about it in the first place). However, many people can appreciate a finely-crafted poem and be inspired with awe and new ideas. When we commoditize and abstract-away something, we also give-up the nuance and expression of semantics. It’s reasonable to make this trade and treat some systems as a “black box”, though it’s also important to appreciate the hidden complexity. To describe data mappings as plumbing is to describe poetry as words – true, though also missing truth. To justify obfuscation of systems is to gatekeep the exchange of ideas and possibly suppress the true passions of others in the process. There are many interesting and noble ongoing projects to solve pieces of this puzzle, and the ideal I hope to contribute towards is this: treat code more like art and less like a commodity, be empathetic and collaborative when doing so, and then everybody wins.

## Postscript

Thank you for making it this far! To be honest, I care most about the semantics of improving the patient and clinician experiences, and this journey into data mappings was somewhat “begrudgingly” started. However, it has been fun and I hope that my work can help people (including myself) in pursuing their semantic journeys. I’m glad I find this problem interesting and am grateful to have the opportunity to explore it rigorously and collaboratively. Onward!

### Post-Postscript

Check out [pydian](https://github.com/ericpan64/pydian): what I hope might be the "ReactJS" for data mappings in Python!

---

## Footnotes

[^0]: The first version of this essay was submitted in March 2024, though I've added progressive updates + cleanup since then. So it's most recently updated with this commit, however the original spirit + message of the essay remains the same. [↩](#data-mapping-is-poetry-not-plumbing)

[^1]: My current department rebranded from “Biomedical Informatics” to “Biomedical Data Science” partially due to this reason (at the time, I also opted for “Biomedical Data Science”). [↩](#i-opinion-data-engineering-and-science-is-about-mapping-semantics)

[^2]: This is semi-formally “feature engineering” in machine learning which I’d consider the same thing, though feature engineering has a specific context of improving a machine learning model. [↩](#i-opinion-data-engineering-and-science-is-about-mapping-semantics)

[^3]: I took a class by Mark Musen (Stanford Biomedical Data Science) on Medical Ontologies (BIOMEDIN 210) which has inspired this thinking, and I love the ethos behind his work with [CEDAR](https://medicine.stanford.edu/2019-report/cedar-to-the-rescue.html). [↩](#ii-motivation-most-of-data-driven-applications-is-data-wrangling)